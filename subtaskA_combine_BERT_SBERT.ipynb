{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combining BERT idiomatic / literal prediction & SBERT ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction, CrossEncoder\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "from evaluation_a import evaluation_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_scores(current, sentences):\n",
    "    # input = current line(example) & embeddings for sentence + captions\n",
    "    \n",
    "    scores = {}\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "\n",
    "    embeddings = model.encode(sentences)\n",
    "    similarities = model.similarity(embeddings[0], embeddings)\n",
    "    # compares the embedding for the description of idiomatic / literal use of compound\n",
    "    # with each of the embeddings, including itself and all the captions\n",
    "\n",
    "    # [0][x] required because similarities tensor has additional layer\n",
    "    score1 = similarities[0][1].item()\n",
    "    scores[current[\"image1_name\"]] = score1\n",
    "\n",
    "    score2 = similarities[0][2].item()\n",
    "    scores[current[\"image2_name\"]] = score2\n",
    "\n",
    "    score3 = similarities[0][3].item()\n",
    "    scores[current[\"image3_name\"]] = score3\n",
    "\n",
    "    score4 = similarities[0][4].item()\n",
    "    scores[current[\"image4_name\"]] = score4\n",
    "\n",
    "    score5 = similarities[0][5].item()\n",
    "    scores[current[\"image5_name\"]] = score5\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def rank_images(scores):\n",
    "    ranking = []\n",
    "    # scores = dictionary containing the cos similarity scores\n",
    "    # comparing the sentence with the captions of the five images\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "\n",
    "    for i in range(5):\n",
    "        # find key which corresponds to the highest value\n",
    "        m = max(scores, key=scores.get)\n",
    "        # add the key (image name) to the ranking\n",
    "        ranking.append(m)\n",
    "        # delete the entry in the dictionary\n",
    "        del scores[m]\n",
    "\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT embedding data von Wiebke\n",
    "dataA = pd.read_pickle(\"dataA_bert-base-uncased_without_CLS_SEP.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idiomatic\n",
      "idiomatic\n"
     ]
    }
   ],
   "source": [
    "# get the predictions for idiomatic / literal\n",
    "\n",
    "current = dataA.iloc[0]\n",
    "print(current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanSecondToLast\"])\n",
    "print(current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanLast4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37142857142857144\n",
      "0.18285714285714283\n"
     ]
    }
   ],
   "source": [
    "# taking information whether idiomatic or literal from BERT decisions\n",
    "\n",
    "data = dataA[(dataA[\"subset\"] == \"Train\") | (dataA[\"subset\"] == \"Sample\")]\n",
    "#data = data[data[\"sentence_type\"]==\"literal\"]\n",
    "#data = data[data[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    # get gpt idiomatic / literal meaning based on BERT prediction\n",
    "    reference = current[\"gpt_idiomatic_meaning\"]\n",
    "    if current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanSecondToLast\"] == \"literal\":\n",
    "        reference = current[\"gpt_literal_meaning\"]\n",
    "\n",
    "    sentences = [reference, \n",
    "                 current[\"image1_caption\"],\n",
    "                 current[\"image2_caption\"],\n",
    "                 current[\"image3_caption\"],\n",
    "                 current[\"image4_caption\"],\n",
    "                 current[\"image5_caption\"]]\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking, exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire training data:\n",
    "#0.37142857142857144\n",
    "#0.18285714285714283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37142857142857144\n",
      "0.18571428571428572\n"
     ]
    }
   ],
   "source": [
    "# comparison with actual sentence type given in training data\n",
    "\n",
    "data = dataA[(dataA[\"subset\"] == \"Train\") | (dataA[\"subset\"] == \"Sample\")]\n",
    "#data = data[data[\"sentence_type\"]==\"literal\"]\n",
    "#data = data[data[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    # get gpt idiomatic / literal meaning based on BERT prediction\n",
    "    reference = current[\"gpt_idiomatic_meaning\"]\n",
    "    if current[\"sentence_type\"] == \"literal\":\n",
    "        reference = current[\"gpt_literal_meaning\"]\n",
    "\n",
    "    sentences = [reference, \n",
    "                 current[\"image1_caption\"],\n",
    "                 current[\"image2_caption\"],\n",
    "                 current[\"image3_caption\"],\n",
    "                 current[\"image4_caption\"],\n",
    "                 current[\"image5_caption\"]]\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking, exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire training data:\n",
    "#0.37142857142857144 -> same as with BERT predictions\n",
    "#0.18571428571428572 -> only slightly better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making results better by doing preprocessing & altering ChatGPT descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function von Victoria\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    \n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    normalized_text = raw_text.lower()\n",
    "    normalized_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", normalized_text)\n",
    "\n",
    "    # Tokenize the normalized text\n",
    "    tokens = word_tokenize(normalized_text)\n",
    "\n",
    "    # Apply POS tagging and retain only nouns, verbs\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    pos_tags_to_keep = {\"NOUN\", \"VERB\"}\n",
    "    filtered_tokens = [word for word, pos in pos_tags if pos in pos_tags_to_keep]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in filtered_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    \n",
    "    return \" \".join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.1842857142857143\n"
     ]
    }
   ],
   "source": [
    "data = dataA[(dataA[\"subset\"] == \"Train\") | (dataA[\"subset\"] == \"Sample\")]\n",
    "#data = data[data[\"sentence_type\"]==\"literal\"]\n",
    "#data = data[data[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    # get gpt idiomatic / literal meaning based on BERT prediction\n",
    "    reference = current[\"gpt_idiomatic_meaning\"]\n",
    "    if current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanSecondToLast\"] == \"literal\":\n",
    "        reference = current[\"gpt_literal_meaning\"]\n",
    "\n",
    "    sentences = [prepare_text(reference), \n",
    "                 prepare_text(current[\"image1_caption\"]),\n",
    "                 prepare_text(current[\"image2_caption\"]),\n",
    "                 prepare_text(current[\"image3_caption\"]),\n",
    "                 prepare_text(current[\"image4_caption\"]),\n",
    "                 prepare_text(current[\"image5_caption\"])]\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking, exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire training data:\n",
    "#0.4\n",
    "#0.1842857142857143\n",
    "\n",
    "# very slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column with cut descriptions\n",
    "new_i = []\n",
    "new_l = []\n",
    "\n",
    "for i in range(len(dataA)):\n",
    "    new_idiomatic = dataA.iloc[i][\"gpt_idiomatic_meaning\"].split(\"is a metaphor for \")[1]\n",
    "    new_i.append(new_idiomatic)\n",
    "    new_literal = dataA.iloc[i][\"gpt_literal_meaning\"].split(\"literal\")[1].strip()\n",
    "    new_l.append(new_literal)\n",
    "\n",
    "dataA[\"gpt_literal_meaning_cut\"] = new_l\n",
    "dataA[\"gpt_idiomatic_meaning_cut\"] = new_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elbow grease is a metaphor for hard physical effort or industrious labor.\n",
      "hard physical effort or industrious labor.\n",
      "Elbow grease is the literal effort of using one's elbow and arm to apply pressure during cleaning or polishing.\n",
      "effort of using one's elbow and arm to apply pressure during cleaning or polishing.\n"
     ]
    }
   ],
   "source": [
    "print(dataA.iloc[0][\"gpt_idiomatic_meaning\"])\n",
    "print(dataA.iloc[0][\"gpt_idiomatic_meaning_cut\"])\n",
    "print(dataA.iloc[0][\"gpt_literal_meaning\"])\n",
    "print(dataA.iloc[0][\"gpt_literal_meaning_cut\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5571428571428572\n",
      "0.12571428571428564\n"
     ]
    }
   ],
   "source": [
    "# no preprocessing, cut meaning\n",
    "\n",
    "data = dataA[(dataA[\"subset\"] == \"Train\") | (dataA[\"subset\"] == \"Sample\")]\n",
    "#data = data[data[\"sentence_type\"]==\"literal\"]\n",
    "#data = data[data[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    # get gpt idiomatic / literal meaning based on BERT prediction\n",
    "    reference = current[\"gpt_idiomatic_meaning_cut\"]\n",
    "    if current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanSecondToLast\"] == \"literal\":\n",
    "        reference = current[\"gpt_literal_meaning_cut\"]\n",
    "\n",
    "    sentences = [reference, \n",
    "                 current[\"image1_caption\"],\n",
    "                 current[\"image2_caption\"],\n",
    "                 current[\"image3_caption\"],\n",
    "                 current[\"image4_caption\"],\n",
    "                 current[\"image5_caption\"]]\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking, exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire training data:\n",
    "\n",
    "#0.5571428571428572\n",
    "#0.12571428571428564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5142857142857142\n",
      "0.1499999999999999\n"
     ]
    }
   ],
   "source": [
    "# cut meaning, preprocessing\n",
    "\n",
    "data = dataA[(dataA[\"subset\"] == \"Train\") | (dataA[\"subset\"] == \"Sample\")]\n",
    "#data = data[data[\"sentence_type\"]==\"literal\"]\n",
    "#data = data[data[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    # get gpt idiomatic / literal meaning based on BERT prediction\n",
    "    reference = current[\"gpt_idiomatic_meaning_cut\"]\n",
    "    if current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanSecondToLast\"] == \"literal\":\n",
    "        reference = current[\"gpt_literal_meaning_cut\"]\n",
    "\n",
    "    sentences = [prepare_text(reference), \n",
    "                 prepare_text(current[\"image1_caption\"]),\n",
    "                 prepare_text(current[\"image2_caption\"]),\n",
    "                 prepare_text(current[\"image3_caption\"]),\n",
    "                 prepare_text(current[\"image4_caption\"]),\n",
    "                 prepare_text(current[\"image5_caption\"])]\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking, exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire training data:\n",
    "#0.5142857142857142\n",
    "#0.1499999999999999\n",
    "\n",
    "# preprocessing worse top1 accuracy, better for Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT data von Julio\n",
    "data_desc = pd.read_csv(\"gpt-desc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5571428571428572\n",
      "0.1357142857142857\n"
     ]
    }
   ],
   "source": [
    "# compare embeddings for description of compound (idiomatic/literal) with image captions\n",
    "\n",
    "data = dataA[(dataA[\"subset\"] == \"Train\") | (dataA[\"subset\"] == \"Sample\")]\n",
    "#data = dataA[dataA[\"sentence_type\"]==\"literal\"]\n",
    "#data = dataA[dataA[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    # using info of current[\"compound\"] & current[\"sentence_type\"], get meaning from chatgpt data data_desc\n",
    "    description = data_desc[(data_desc[\"compound\"]==current[\"compound\"]) \n",
    "                            & (data_desc[\"sentence_type\"]==current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanSecondToLast\"])][\"Meaning\"].item()\n",
    "    sentences = [description, \n",
    "                 current[\"image1_caption\"],\n",
    "                 current[\"image2_caption\"],\n",
    "                 current[\"image3_caption\"],\n",
    "                 current[\"image4_caption\"],\n",
    "                 current[\"image5_caption\"]]\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking,exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire training data\n",
    "#0.5571428571428572\n",
    "#0.1357142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as submission_EN.tsv\n"
     ]
    }
   ],
   "source": [
    "data = dataA[dataA[\"subset\"] == \"Dev\"]\n",
    "\n",
    "submission = []\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    # get gpt idiomatic / literal meaning based on BERT prediction\n",
    "    reference = current[\"gpt_idiomatic_meaning_cut\"]\n",
    "    if current[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanSecondToLast\"] == \"literal\":\n",
    "        reference = current[\"gpt_literal_meaning_cut\"]\n",
    "\n",
    "    sentences = [reference, \n",
    "                 current[\"image1_caption\"],\n",
    "                 current[\"image2_caption\"],\n",
    "                 current[\"image3_caption\"],\n",
    "                 current[\"image4_caption\"],\n",
    "                 current[\"image5_caption\"]]\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    submission.append({\n",
    "        \"compound\": current[\"compound\"],\n",
    "        \"expected_order\":ranking \n",
    "    })\n",
    "\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv(\"submission_EN.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"File saved as submission_EN.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
