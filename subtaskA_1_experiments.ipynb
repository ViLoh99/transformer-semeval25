{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approaching subtask A using sentence embeddings\n",
    "\n",
    "- random baseline\n",
    "- ranking based on similarity of sentence embeddings for the compound / sentence and image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv file\n",
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "dataDirectory = \"./data/\"\n",
    "\n",
    "# read in competition data\n",
    "dataA_train = pd.read_csv(dataDirectory + \"subtask_a_train.tsv\", sep='\\t')\n",
    "dataA_train['expected_order'] = dataA_train['expected_order'].apply(ast.literal_eval)\n",
    "dataA_dev = pd.read_csv(dataDirectory + \"subtask_a_dev.tsv\", sep='\\t')\n",
    "dataA_test = pd.read_csv(dataDirectory +\"subtask_a_test.tsv\", sep='\\t')\n",
    "\n",
    "dataA = pd.concat([dataA_train,dataA_dev,dataA_test])\n",
    "# reset index\n",
    "dataA = dataA.reset_index(drop=True)\n",
    "\n",
    "# read in chatGPT data from csv\n",
    "data_chatGPT_train = pd.read_csv(dataDirectory + \"chatGPTNew_train.csv\")\n",
    "data_chatGPT_dev = pd.read_csv(dataDirectory + \"chatGPTNew_dev.csv\")\n",
    "data_chatGPT_test = pd.read_csv(dataDirectory + \"chatGPTNew_test.csv\")\n",
    "data_chatGPT = pd.concat([data_chatGPT_train,data_chatGPT_dev,data_chatGPT_test])\n",
    "\n",
    "data_chatGPT = data_chatGPT.reset_index(drop=True)\n",
    "\n",
    "# rename each column with \"gpt_\" in front of the column name\n",
    "data_chatGPT.rename(columns=lambda x: 'gpt_' + x, inplace=True)\n",
    "\n",
    "# inserting the missing compound column\n",
    "data_chatGPT[\"compound\"] = [None for i in range(len(data_chatGPT))]\n",
    "for i in range(len(data_chatGPT)):\n",
    "    data_chatGPT[\"compound\"][i] = data_chatGPT[\"gpt_idiomatic_meaning\"][i].split(\" is\")[0].strip().lower()\n",
    "\n",
    "# read in gpt image description data\n",
    "data_gpt_image = pd.read_csv(dataDirectory  + \"gpt_image_descriptions_all.csv\", sep=',')\n",
    "\n",
    "# merge data into one dataframe\n",
    "dataA = pd.merge(dataA, data_chatGPT, on='compound')\n",
    "dataA = pd.merge(dataA, data_gpt_image, on='compound')\n",
    "\n",
    "sentence_type_columns = ['sentence', \n",
    "                         'image1_caption', 'image2_caption', 'image3_caption', 'image4_caption', 'image5_caption', \n",
    "                         'gpt_idiomatic_meaning', 'gpt_literal_meaning', \n",
    "                         'gpt_idiomatic_sentence', 'gpt_literal_sentence',\n",
    "                         'gpt_idiomatic_image', 'gpt_literal_image']\n",
    "\n",
    "\n",
    "# cleanup data\n",
    "# replace ’ with ' in all columns\n",
    "for column in sentence_type_columns:\n",
    "    dataA[column] = dataA[column].str.replace(\"’\",\"'\")\n",
    "\n",
    "\n",
    "preprocessed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "def only_train(dataA): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    return pd.concat([dataA[dataA[\"subset\"] == \"Sample\"],dataA[dataA[\"subset\"]== \"Train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dataframe of subset\n",
    "def only_subset(dataA, subset): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    return dataA[dataA[\"subset\"] == subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in predicted idiomatic/literal prediction\n",
    "\n",
    "dataA[\"binary_pred\"] = pd.read_pickle(\"binary_pred.pkl\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "dataA_train = only_train(dataA)\n",
    "accuracy_score(dataA_train[\"binary_pred\"],dataA_train[\"sentence_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Wiebke Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# preprocessing of text (from Victoria)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    \n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    normalized_text = raw_text.lower()\n",
    "    normalized_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", normalized_text)\n",
    "\n",
    "    # Tokenize the normalized text\n",
    "    tokens = word_tokenize(normalized_text)\n",
    "\n",
    "    # Apply POS tagging and retain only nouns, verbs\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    pos_tags_to_keep = {\"NOUN\", \"VERB\"}\n",
    "    filtered_tokens = [word for word, pos in pos_tags if pos in pos_tags_to_keep]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in filtered_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    \n",
    "    return \" \".join(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if preprocessed == True:\n",
    "    dataA[\"compound\"] = dataA[\"compound\"].apply(prepare_text)\n",
    "    for column in sentence_type_columns:\n",
    "        dataA[column] = dataA[column].apply(prepare_text) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to display images\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "fileDirectory = 'D:\\\\Wiebke Petersen\\\\Downloads\\\\AdMIRe Subtask A Train\\\\train'\n",
    "\n",
    "# Open the image file\n",
    "def display_image(compound, fn):\n",
    "    img = Image.open(fileDirectory + \"\\\\\" + compound + \"\\\\\" + fn)\n",
    "    new_size = (150, 150)  # Width, Height\n",
    "    img_resized = img.resize(new_size)  \n",
    "    # Display the image\n",
    "    display(img_resized)\n",
    "\n",
    "# returns list of image names sorted from image1 to image5\n",
    "def get_image_names(n,mydata):\n",
    "    names = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "         names.append(mydata['image' + str(i) + '_name'][n])\n",
    "    return names\n",
    "\n",
    "# print information of 1 item:\n",
    "\n",
    "def print_item(n, mydata):\n",
    "    # print  'sentence_type', 'sentence'\n",
    "    compound = mydata['compound'][n]\n",
    "    print(compound)\n",
    "    print(mydata['sentence_type'][n])\n",
    "    print(mydata['sentence'][n])\n",
    "    print('---------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # for image_names in 'expected_order' print image_captions\n",
    "    names  =  get_image_names(n,mydata)\n",
    "    expected_order = mydata['expected_order'][n]\n",
    "    print(expected_order)\n",
    "\n",
    "    for image_name in expected_order:\n",
    "        display_image(compound, image_name)\n",
    "        # get index of image_name in names\n",
    "        index = names.index(image_name) + 1\n",
    "        print(mydata['image'+str(index)+'_caption'][n])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in sentence_type_columns:\n",
    "    dataA[type + \"_sbert_embedding\"] = dataA[type].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if preprocessed == True:\n",
    "    prep = \"_preprocessed_\"\n",
    "else:\n",
    "    prep = \"_\"\n",
    "\n",
    "dataA.to_pickle(\"dataA_sbert\"+ prep  + \".pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = False\n",
    "\n",
    "if preprocessed == True:\n",
    "    prep = \"_preprocessed_\"\n",
    "else:\n",
    "    prep = \"_\"\n",
    "\n",
    "dataA = pd.read_pickle(\"dataA_sbert\"+ prep + \".pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# evaluation functions for ranked orders\n",
    "def top1accuracy(pred_rankings,expected_order):\n",
    "    correct = 0\n",
    "    for i in range(len(pred_rankings)):\n",
    "        if pred_rankings[i][0] == expected_order[i][0]:\n",
    "            correct += 1\n",
    "    return round(correct/len(pred_rankings),3)\n",
    "\n",
    "\n",
    "def spearman_correlation(pred_rankings,expected_order):\n",
    "    corr = []\n",
    "    for i in range(len(pred_rankings)):\n",
    "        corr.append(spearmanr(pred_rankings[i],expected_order[i]).correlation)\n",
    "    return round(np.mean(corr),3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_scores(current, comparator):\n",
    "    # input = current line(example) & embeddings for sentence + captions\n",
    "    \n",
    "    scores = {}\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "    embeddings = [current[comparator+ \"_sbert_embedding\"], \n",
    "                 current[\"image1_caption_sbert_embedding\"],\n",
    "                 current[\"image2_caption_sbert_embedding\"],\n",
    "                 current[\"image3_caption_sbert_embedding\"],\n",
    "                 current[\"image4_caption_sbert_embedding\"],\n",
    "                 current[\"image5_caption_sbert_embedding\"]]\n",
    "\n",
    "    #embeddings = model.encode(sentences)\n",
    "    similarities = model.similarity(embeddings[0], embeddings)\n",
    "    # compares the embedding for the sentence including the compound \n",
    "    # with each of the embeddings, including itself and all the captions\n",
    "\n",
    "    # [0][x] required because similarities tensor has additional layer\n",
    "    score1 = similarities[0][1].item()\n",
    "    scores[current[\"image1_name\"]] = score1\n",
    "\n",
    "    score2 = similarities[0][2].item()\n",
    "    scores[current[\"image2_name\"]] = score2\n",
    "\n",
    "    score3 = similarities[0][3].item()\n",
    "    scores[current[\"image3_name\"]] = score3\n",
    "\n",
    "    score4 = similarities[0][4].item()\n",
    "    scores[current[\"image4_name\"]] = score4\n",
    "\n",
    "    score5 = similarities[0][5].item()\n",
    "    scores[current[\"image5_name\"]] = score5\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_images(scores):\n",
    "    ranking = []\n",
    "    # scores = dictionary containing the cos similarity scores\n",
    "    # comparing the sentence with the captions of the five images\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "\n",
    "    for i in range(5):\n",
    "        # find key which corresponds to the highest value\n",
    "        m = max(scores, key=scores.get)\n",
    "        # add the key (image name) to the ranking\n",
    "        ranking.append(m)\n",
    "        # delete the entry in the dictionary\n",
    "        del scores[m]\n",
    "\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependent_preds(current, comparator):\n",
    "    pred = current[\"binary_pred\"]\n",
    "    return rank_images(sim_scores(current, \"gpt_\" + pred + \"_\" + comparator ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on training data (rank images by similarity to sentence):\n",
      "top1 accuracy 0.4\n",
      "spearman correlation 0.201\n"
     ]
    }
   ],
   "source": [
    "dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, \"sentence\")), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"Evaluation on training data (rank images by similarity to sentence):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_sentence\n",
      "top1 accuracy 0.329\n",
      "spearman correlation 0.054\n",
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_meaning\n",
      "top1 accuracy 0.371\n",
      "spearman correlation 0.183\n",
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_image\n",
      "top1 accuracy 0.557\n",
      "spearman correlation 0.086\n"
     ]
    }
   ],
   "source": [
    "for sent_type in ['sentence', 'meaning','image']:\n",
    "    print(\"\\nDepending on binary prediction (literal/idiomatic) rank images by similarity to gpt_\" + sent_type)\n",
    "    dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds(x,sent_type), axis=1)\n",
    "\n",
    "    dataA_train = only_train(dataA)\n",
    "    print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "    print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['compound', 'subset', 'sentence_type', 'sentence', 'expected_order',\n",
       "       'image1_name', 'image1_caption', 'image2_name', 'image2_caption',\n",
       "       'image3_name', 'image3_caption', 'image4_name', 'image4_caption',\n",
       "       'image5_name', 'image5_caption', 'gpt_idiomatic_meaning',\n",
       "       'gpt_literal_meaning', 'gpt_idiomatic_sentence', 'gpt_literal_sentence',\n",
       "       'gpt_idiomatic_image', 'gpt_literal_image', 'binary_pred',\n",
       "       'sentence_sbert_embedding', 'image1_caption_sbert_embedding',\n",
       "       'image2_caption_sbert_embedding', 'image3_caption_sbert_embedding',\n",
       "       'image4_caption_sbert_embedding', 'image5_caption_sbert_embedding',\n",
       "       'gpt_idiomatic_meaning_sbert_embedding',\n",
       "       'gpt_literal_meaning_sbert_embedding',\n",
       "       'gpt_idiomatic_sentence_sbert_embedding',\n",
       "       'gpt_literal_sentence_sbert_embedding',\n",
       "       'gpt_idiomatic_image_sbert_embedding',\n",
       "       'gpt_literal_image_sbert_embedding', 'pred_order',\n",
       "       'pred_order_dependent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim scores\n",
    "\n",
    "def dependent_preds(current, sent_type):\n",
    "       preds = [0 for i in range(5)]\n",
    "       scores_lit = sim_scores(current, \"gpt_literal_\" + sent_type)\n",
    "       scores_id = sim_scores(current, \"gpt_idiomatic_\" + sent_type)\n",
    "       image_names = list(scores_lit.keys())\n",
    "       # get highest scoring image for literal and idiomatic\n",
    "       m_lit = max(scores_lit,key=scores_lit.get)\n",
    "       preds[0] = m_lit\n",
    "\n",
    "       del scores_id[m_lit]\n",
    "       m_id = max(scores_id, key=scores_id.get)\n",
    "       preds[3] = m_id\n",
    "    \n",
    "       m_lit_index = image_names.index(m_lit)\n",
    "       m_id_index = image_names.index(m_id)\n",
    "       scores_images_lit = sim_scores(current, \"image\" + str(m_lit_index + 1) + \"_caption\")\n",
    "       scores_images_id = sim_scores(current, \"image\" + str(m_id_index +1) + \"_caption\")\n",
    "       del scores_images_lit[m_lit]\n",
    "       del scores_images_lit[m_id]\n",
    "       del scores_images_id[m_lit]\n",
    "       del scores_images_id[m_id]\n",
    "\n",
    "       sim_max_lit = max(scores_images_lit, key=scores_images_lit.get)\n",
    "       preds[1] = sim_max_lit\n",
    "    \n",
    "       del scores_images_id[sim_max_lit]\n",
    "       sim_max_id = max(scores_images_id, key=scores_images_id.get)\n",
    "       preds[2] = sim_max_id\n",
    "       preds[4] = list(set(image_names).difference(set([m_lit,m_id,sim_max_lit,sim_max_id])))[0]\n",
    "       if not(set(preds) == set(image_names)):\n",
    "           print(\"there is some serious problem\") \n",
    "       if current[\"binary_pred\"] == \"idiomatic\":\n",
    "          preds_new = [preds[i] for i in [3,2,1,0,4]]\n",
    "          preds = preds_new\n",
    "       return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on training data (rank images dependent on binary classification. For order use inter-image similarity):\n",
      "top1 accuracy 0.543\n",
      "spearman correlation 0.286\n"
     ]
    }
   ],
   "source": [
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds(x,\"image\"), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"Evaluation on training data (rank images dependent on binary classification. For order use inter-image similarity):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as submission_EN.tsv\n"
     ]
    }
   ],
   "source": [
    "def make_submission(dataA, column, subset):\n",
    "    subset_data = only_subset(dataA,subset)\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"compound\"] = subset_data[\"compound\"]\n",
    "    submission_df[\"expected_order\"] = subset_data[column]\n",
    "    submission_df.to_csv(\"submission_EN.tsv\", sep=\"\\t\", index=False)\n",
    "    print(\"File saved as submission_EN.tsv\")\n",
    "\n",
    "make_submission(dataA,\"pred_order_dependent\", \"Dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim scores\n",
    "\n",
    "def dependent_preds_no_pairs(current, sent_type):\n",
    "    preds = [0 for i in range(5)]\n",
    "    scores_lit = sim_scores(current, \"gpt_literal_\" + sent_type)\n",
    "    scores_id = sim_scores(current, \"gpt_idiomatic_\" + sent_type)\n",
    "    image_names = list(scores_lit.keys())\n",
    "    # get highest scoring image for literal and idiomatic\n",
    "    m_lit = max(scores_lit,key=scores_lit.get)\n",
    "    preds[0] = m_lit\n",
    "\n",
    "    del scores_id[m_lit]\n",
    "    m_id = max(scores_id, key=scores_id.get)\n",
    "    preds[3] = m_id\n",
    "\n",
    "    del scores_lit[m_lit]\n",
    "    del scores_lit[m_id]\n",
    "\n",
    "    m_lit_second = max(scores_lit, key=scores_lit.get)\n",
    "    preds[1] = m_lit_second\n",
    "\n",
    "    del scores_id[m_id]\n",
    "    del scores_id[m_lit_second]\n",
    "    m_id_second = max(scores_id, key=scores_id.get)\n",
    "    preds[2] = m_id_second\n",
    "    preds[4] = list(set(image_names).difference(set([m_lit,m_id,m_lit_second,m_id_second])))[0]\n",
    "    if not(set(preds) == set(image_names)):\n",
    "           print(\"there is some serious problem\") \n",
    "    if current[\"binary_pred\"] == \"idiomatic\":\n",
    "          preds_new = [preds[i] for i in [3,2,1,0,4]]\n",
    "          preds = preds_new\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on training data (rank images dependent on binary classification. For order use similarity to gpt_image only):\n",
      "top1 accuracy 0.543\n",
      "spearman correlation 0.283\n"
     ]
    }
   ],
   "source": [
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds_no_pairs(x,\"image\"), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"Evaluation on training data (rank images dependent on binary classification. For order use similarity to gpt_image only):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence\n",
    "\n",
    "# entire subtask A training data (70):\n",
    "# final_acc = 0.4\n",
    "# final_spearman = 0.20142857142857135\n",
    "\n",
    "# only literal (31):\n",
    "# final_acc = 0.6774193548387096\n",
    "# final_spearman = 0.3064516129032258\n",
    "\n",
    "# only idiomatic (39):\n",
    "# final_acc = 0.1794871794871795\n",
    "# final_spearman = 0.11794871794871792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m current \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m     13\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m     14\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage1_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     15\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage2_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     16\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage3_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     17\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage4_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage5_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m---> 20\u001b[0m scores \u001b[38;5;241m=\u001b[39m sim_scores(current, sentences)\n\u001b[0;32m     21\u001b[0m ranking \u001b[38;5;241m=\u001b[39m rank_images(scores)\n\u001b[0;32m     23\u001b[0m exp_order \u001b[38;5;241m=\u001b[39m current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_order\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36msim_scores\u001b[1;34m(current, comparator)\u001b[0m\n\u001b[0;32m      4\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# keys = image names\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# values = scores\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [current[comparator\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_sbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      8\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage1_caption_sbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage2_caption_sbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage3_caption_sbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     11\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage4_caption_sbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     12\u001b[0m              current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage5_caption_sbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#embeddings = model.encode(sentences)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m similarities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msimilarity(embeddings[\u001b[38;5;241m0\u001b[39m], embeddings)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "# comparing embeddings for captions with that of compound (not whole sentence)\n",
    "\n",
    "data = dataA\n",
    "#data = dataA[dataA[\"sentence_type\"]==\"literal\"]\n",
    "#data = dataA[dataA[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    scores = sim_scores(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking,exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compound\n",
    "\n",
    "#  entire subtask A training data (70):\n",
    "# final_acc = 0.4\n",
    "# final_spearman = 0.16714285714285712\n",
    "\n",
    "# only literal (31):\n",
    "# final_acc = 0.8064516129032258\n",
    "# final_spearman = 0.39354838709677425\n",
    "\n",
    "# only idiomatic (39):\n",
    "# final_acc = 0.07692307692307693\n",
    "# final_spearman = -0.012820512820512832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining \"sentence\" and \"compound\" embeddings (average)\n",
    "\n",
    "def sim_scores_combined(current, sentences):\n",
    "    # input = current line(example) & embeddings for sentence + captions\n",
    "    \n",
    "    scores = {}\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    # combining compound & sentence embeddings\n",
    "    sent_comp = (embeddings[0] + embeddings[1]) / 2\n",
    "\n",
    "    similarities = model.similarity(sent_comp, embeddings[2:])\n",
    "    # compares the embedding for the sentence and compound combined\n",
    "    # with each of the embeddings, including itself and all the captions\n",
    "\n",
    "    # [0][x] required because similarities tensor has additional layer\n",
    "    score1 = similarities[0][0].item()\n",
    "    scores[current[\"image1_name\"]] = score1\n",
    "\n",
    "    score2 = similarities[0][1].item()\n",
    "    scores[current[\"image2_name\"]] = score2\n",
    "\n",
    "    score3 = similarities[0][2].item()\n",
    "    scores[current[\"image3_name\"]] = score3\n",
    "\n",
    "    score4 = similarities[0][3].item()\n",
    "    scores[current[\"image4_name\"]] = score4\n",
    "\n",
    "    score5 = similarities[0][4].item()\n",
    "    scores[current[\"image5_name\"]] = score5\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34285714285714286\n",
      "0.2271428571428571\n"
     ]
    }
   ],
   "source": [
    "# comparing embeddings for captions with that of compound and sentence (averaged)\n",
    "\n",
    "data = dataA\n",
    "#data = dataA[dataA[\"sentence_type\"]==\"literal\"]\n",
    "#data = dataA[dataA[\"sentence_type\"]==\"idiomatic\"]\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "    sentences = [current[\"compound\"], \n",
    "                 current[\"sentence\"], \n",
    "                 current[\"image1_caption\"],\n",
    "                 current[\"image2_caption\"],\n",
    "                 current[\"image3_caption\"],\n",
    "                 current[\"image4_caption\"],\n",
    "                 current[\"image5_caption\"]]\n",
    "\n",
    "    scores = sim_scores_combined(current, sentences)\n",
    "    ranking = rank_images(scores)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking,exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and compound combined\n",
    "\n",
    "# entire subtask A training data:\n",
    "# final_acc = 0.34285714285714286 -> worse than compound / sentence individually\n",
    "# final_spearman = 0.2271428571428571 -> better than either individually\n",
    "\n",
    "# only literal: -> worse than compound, better than sentence\n",
    "# final_acc = 0.7096774193548387 \n",
    "# final_spearman = 0.36774193548387096\n",
    "\n",
    "# only idiomatic:\n",
    "# final_acc = 0.05128205128205128 -> worse than either individually\n",
    "# final_spearman = 0.11538461538461536 -> better than compound, about as good as sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17142857142857143\n",
      "0.03428571428571425\n"
     ]
    }
   ],
   "source": [
    "# random baseline\n",
    "\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "data = dataA\n",
    "\n",
    "total_acc = 0\n",
    "total_spearman = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    current = data.iloc[i]\n",
    "\n",
    "    ranking = []\n",
    "    images = [current[\"image1_name\"],\n",
    "              current[\"image2_name\"],\n",
    "              current[\"image3_name\"],\n",
    "              current[\"image4_name\"],\n",
    "              current[\"image5_name\"]]\n",
    "\n",
    "    for i in range(5):\n",
    "        rand_img = random.choice(images)\n",
    "        ranking.append(rand_img)\n",
    "        images.remove(rand_img)\n",
    "\n",
    "    exp_order = current[\"expected_order\"]\n",
    "    evaluation = evaluation_single(ranking,exp_order)\n",
    "    total_acc += evaluation[0]\n",
    "    total_spearman += evaluation[1]\n",
    "\n",
    "final_acc = total_acc / len(data)\n",
    "print(final_acc)\n",
    "final_spearman = total_spearman / len(data)\n",
    "print(final_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random\n",
    "\n",
    "# final_acc = 0.17142857142857143\n",
    "# final_spearman = 0.03428571428571425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
