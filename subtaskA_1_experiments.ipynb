{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approaching subtask A using sentence embeddings\n",
    "\n",
    "- random baseline\n",
    "- ranking based on similarity of sentence embeddings for the compound / sentence and image captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** needs binar_pred.pkl file generated in subtaskA_predictions_fromBERT.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv file\n",
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "dataDirectory = \"./data/\"\n",
    "\n",
    "# read in competition data\n",
    "dataA_train = pd.read_csv(dataDirectory + \"subtask_a_train.tsv\", sep='\\t')\n",
    "dataA_train['expected_order'] = dataA_train['expected_order'].apply(ast.literal_eval)\n",
    "dataA_dev = pd.read_csv(dataDirectory + \"subtask_a_dev.tsv\", sep='\\t')\n",
    "dataA_xe = pd.read_csv(dataDirectory + \"subtask_a_xe.tsv\", sep='\\t')\n",
    "dataA_test = pd.read_csv(dataDirectory +\"subtask_a_test.tsv\", sep='\\t')\n",
    "\n",
    "dataA = pd.concat([dataA_train,dataA_dev,dataA_test,dataA_xe])\n",
    "# reset index\n",
    "dataA = dataA.reset_index(drop=True)\n",
    "\n",
    "# read in chatGPT data from csv\n",
    "data_chatGPT_train = pd.read_csv(dataDirectory + \"chatGPTNew_train.csv\")\n",
    "data_chatGPT_dev = pd.read_csv(dataDirectory + \"chatGPTNew_dev.csv\")\n",
    "data_chatGPT_test = pd.read_csv(dataDirectory + \"chatGPTNew_test.csv\")\n",
    "data_chatGPT = pd.concat([data_chatGPT_train,data_chatGPT_dev,data_chatGPT_test])\n",
    "\n",
    "data_chatGPT = data_chatGPT.reset_index(drop=True)\n",
    "\n",
    "# rename each column with \"gpt_\" in front of the column name\n",
    "data_chatGPT.rename(columns=lambda x: 'gpt_' + x, inplace=True)\n",
    "\n",
    "# inserting the missing compound column\n",
    "data_chatGPT[\"compound\"] = [None for i in range(len(data_chatGPT))]\n",
    "for i in range(len(data_chatGPT)):\n",
    "    data_chatGPT[\"compound\"][i] = data_chatGPT[\"gpt_idiomatic_meaning\"][i].split(\" is\")[0].strip().lower()\n",
    "\n",
    "# read in gpt image description data\n",
    "data_gpt_image = pd.read_csv(dataDirectory  + \"gpt_image_descriptions_all.csv\", sep=',')\n",
    "\n",
    "# merge data into one dataframe\n",
    "dataA = pd.merge(dataA, data_chatGPT, on='compound')\n",
    "dataA = pd.merge(dataA, data_gpt_image, on='compound')\n",
    "\n",
    "sentence_type_columns = ['sentence', \n",
    "                         'image1_caption', 'image2_caption', 'image3_caption', 'image4_caption', 'image5_caption', \n",
    "                         'gpt_idiomatic_meaning', 'gpt_literal_meaning', \n",
    "                         'gpt_idiomatic_sentence', 'gpt_literal_sentence',\n",
    "                         'gpt_idiomatic_image', 'gpt_literal_image']\n",
    "\n",
    "\n",
    "# cleanup data\n",
    "# replace ’ with ' in all columns\n",
    "for column in sentence_type_columns:\n",
    "    dataA[column] = dataA[column].str.replace(\"’\",\"'\")\n",
    "\n",
    "\n",
    "preprocessed = False\n",
    "#preprocessed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "def only_train(dataA): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    return pd.concat([dataA[dataA[\"subset\"] == \"Sample\"],dataA[dataA[\"subset\"]== \"Train\"]])\n",
    "\n",
    "# returns the dataframe of subset\n",
    "def only_subset(dataA, subset): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    return dataA[dataA[\"subset\"] == subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor cheating: add opposite sentence_type for each compound in extended data that is also in train/sample\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "dataA_train = dataA_train.reset_index(drop=True)\n",
    "train_compounds = [(dataA_train[\"compound\"][i], dataA_train[\"sentence_type\"][i], list(dataA_train[\"expected_order\"][i])) for i in range(len(dataA_train))]\n",
    "def cheat_items(item):\n",
    "    new_order = [3,2,1,0,4]\n",
    "    if item[\"subset\"] == \"Extended Evaluation\":\n",
    "        compound = item[\"compound\"]\n",
    "        t = [triple for triple in train_compounds if triple[0] == compound]\n",
    "        if t != []:\n",
    "            (c,sent_type,order) = t[0]\n",
    "            if sent_type == \"literal\":\n",
    "                item[\"sentence_type\"] = \"idiomatic\"\n",
    "            else: \n",
    "                item[\"sentence_type\"] = \"literal\"\n",
    "            item[\"expected_order\"] = [order[i] for i in new_order]\n",
    "        else: \n",
    "            item[\"sentence_type\"] = None\n",
    "            item[\"expected_order\"] = None\n",
    "    return item\n",
    "\n",
    "dataA = dataA.apply(lambda x: cheat_items(x), axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9857142857142858\n",
      "0.9357142857142857\n"
     ]
    }
   ],
   "source": [
    "# read in predicted idiomatic/literal prediction\n",
    "# best prediction gained from BERT-embeddings\n",
    "\n",
    "dataA[\"binary_pred\"] = pd.read_pickle(\"binary_pred.pkl\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(accuracy_score(dataA_train[\"binary_pred\"].tolist(),dataA_train[\"sentence_type\"].tolist()))\n",
    "dataA_trainext = dataA[dataA[\"sentence_type\"].notnull()] \n",
    "print(accuracy_score(dataA_trainext[\"binary_pred\"].tolist(),dataA_trainext[\"sentence_type\"].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Wiebke\n",
      "[nltk_data]     Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Wiebke Petersen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# preprocessing of text (from Victoria)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    \n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    normalized_text = raw_text.lower()\n",
    "    normalized_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", normalized_text)\n",
    "\n",
    "    # Tokenize the normalized text\n",
    "    tokens = word_tokenize(normalized_text)\n",
    "\n",
    "    # Apply POS tagging and retain only nouns, verbs\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    pos_tags_to_keep = {\"NOUN\", \"VERB\"}\n",
    "    filtered_tokens = [word for word, pos in pos_tags if pos in pos_tags_to_keep]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in filtered_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    \n",
    "    return \" \".join(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if preprocessed == True:\n",
    "    for column in sentence_type_columns:\n",
    "        dataA[column] = dataA[column].apply(prepare_text) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to display images\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "fileDirectory = 'D:\\\\Wiebke Petersen\\\\Downloads\\\\AdMIRe Subtask A Train\\\\train'\n",
    "\n",
    "# Open the image file\n",
    "def display_image(compound, fn):\n",
    "    img = Image.open(fileDirectory + \"\\\\\" + compound + \"\\\\\" + fn)\n",
    "    new_size = (150, 150)  # Width, Height\n",
    "    img_resized = img.resize(new_size)  \n",
    "    # Display the image\n",
    "    display(img_resized)\n",
    "\n",
    "# returns list of image names sorted from image1 to image5\n",
    "def get_image_names(n,mydata):\n",
    "    names = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "         names.append(mydata['image' + str(i) + '_name'][n])\n",
    "    return names\n",
    "\n",
    "# print information of 1 item:\n",
    "\n",
    "def print_item(n, mydata):\n",
    "    # print  'sentence_type', 'sentence'\n",
    "    compound = mydata['compound'][n]\n",
    "    print(compound)\n",
    "    print(mydata['sentence_type'][n])\n",
    "    print(mydata['sentence'][n])\n",
    "    print('---------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # for image_names in 'expected_order' print image_captions\n",
    "    names  =  get_image_names(n,mydata)\n",
    "    expected_order = mydata['expected_order'][n]\n",
    "    print(expected_order)\n",
    "\n",
    "    for image_name in expected_order:\n",
    "        display_image(compound, image_name)\n",
    "        # get index of image_name in names\n",
    "        index = names.index(image_name) + 1\n",
    "        print(mydata['image'+str(index)+'_caption'][n])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SBert embeddings are generated  for all sentence like columns\n",
    "for type in sentence_type_columns:\n",
    "    dataA[type + \"_sbert_embedding\"] = dataA[type].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if preprocessed == True:\n",
    "    prep = \"_preprocessed_\"\n",
    "else:\n",
    "    prep = \"_\"\n",
    "\n",
    "dataA.to_pickle(\"dataA_sbert\"+ prep  + \".pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#preprocessed = False\n",
    "preprocessed = True # better results\n",
    "\n",
    "if preprocessed == True:\n",
    "    prep = \"_preprocessed_\"\n",
    "else:\n",
    "    prep = \"_\"\n",
    "\n",
    "dataA = pd.read_pickle(\"dataA_sbert\"+ prep + \".pkl\")\n",
    "\n",
    "f = open('results_rankings.txt', 'a')\n",
    "f.write(\"\\n\"+ \"=====================================================================\")\n",
    "f.write(\"\\n\" + \"Ranking results\")\n",
    "if prep == \"_preprocessed_\":\n",
    "    f.write(\"\\n\" +\"preprocessed: True \" + \"noun, verb\")\n",
    "else: \n",
    "    f.write(\"\\n\" +\"preprocessed: False\")\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates submission file from column for subset\n",
    "from zipfile import ZipFile\n",
    "def make_submission(dataA, column, subset):\n",
    "    subset_data = only_subset(dataA,subset)\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"compound\"] = subset_data[\"compound\"]\n",
    "    submission_df[\"expected_order\"] = subset_data[column]\n",
    "    submission_df.to_csv(\"submission_EN.tsv\", sep=\"\\t\", index=False)\n",
    "    ZipFile('submission_EN.zip', 'w').write('submission_EN.tsv')\n",
    "    print(\"File zipped and saved as submission_EN.zip\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# evaluation functions for ranked orders\n",
    "def top1accuracy(pred_rankings,expected_order):\n",
    "    pred_rankings = pred_rankings.to_list()\n",
    "    expected_order = expected_order.to_list()\n",
    "    correct = 0\n",
    "    for i in range(len(pred_rankings)):\n",
    "        if pred_rankings[i][0] == expected_order[i][0]:\n",
    "            correct += 1\n",
    "    return round(correct/len(pred_rankings),3)\n",
    "\n",
    "\n",
    "def spearman_correlation(pred_rankings,expected_order):\n",
    "    pred_rankings = pred_rankings.to_list()\n",
    "    expected_order = expected_order.to_list()\n",
    "    corr = []\n",
    "    for i in range(len(pred_rankings)):\n",
    "        corr.append(spearmanr(pred_rankings[i],expected_order[i]).correlation)\n",
    "    return round(np.mean(corr),3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_scores(current, comparator):\n",
    "    # input = current line(example) & embeddings for sentence + captions\n",
    "    \n",
    "    scores = {}\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "    embeddings = [current[comparator+ \"_sbert_embedding\"], \n",
    "                 current[\"image1_caption_sbert_embedding\"],\n",
    "                 current[\"image2_caption_sbert_embedding\"],\n",
    "                 current[\"image3_caption_sbert_embedding\"],\n",
    "                 current[\"image4_caption_sbert_embedding\"],\n",
    "                 current[\"image5_caption_sbert_embedding\"]]\n",
    "\n",
    "    #embeddings = model.encode(sentences)\n",
    "    similarities = model.similarity(embeddings[0], embeddings)\n",
    "    # compares the embedding for the sentence including the compound \n",
    "    # with each of the embeddings, including itself and all the captions\n",
    "\n",
    "    # [0][x] required because similarities tensor has additional layer\n",
    "    score1 = similarities[0][1].item()\n",
    "    scores[current[\"image1_name\"]] = score1\n",
    "\n",
    "    score2 = similarities[0][2].item()\n",
    "    scores[current[\"image2_name\"]] = score2\n",
    "\n",
    "    score3 = similarities[0][3].item()\n",
    "    scores[current[\"image3_name\"]] = score3\n",
    "\n",
    "    score4 = similarities[0][4].item()\n",
    "    scores[current[\"image4_name\"]] = score4\n",
    "\n",
    "    score5 = similarities[0][5].item()\n",
    "    scores[current[\"image5_name\"]] = score5\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_images(scores):\n",
    "    ranking = []\n",
    "    # scores = dictionary containing the cos similarity scores\n",
    "    # comparing the sentence with the captions of the five images\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "\n",
    "    for i in range(5):\n",
    "        # find key which corresponds to the highest value\n",
    "        m = max(scores, key=scores.get)\n",
    "        # add the key (image name) to the ranking\n",
    "        ranking.append(m)\n",
    "        # delete the entry in the dictionary\n",
    "        del scores[m]\n",
    "\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependent_preds(current, comparator):\n",
    "    pred = current[\"binary_pred\"]\n",
    "    return rank_images(sim_scores(current, \"gpt_\" + pred + \"_\" + comparator ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on training data (rank images by similarity to original sentence):\n",
      "top1 accuracy 0.414\n",
      "spearman correlation 0.164\n",
      "File zipped and saved as submission_EN.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiebke Petersen\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\util.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  a = torch.tensor(a)\n"
     ]
    }
   ],
   "source": [
    "dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, \"sentence\")), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"Evaluation on training data (rank images by similarity to original sentence):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order\"]))\n",
    "\n",
    "make_submission(dataA,\"pred_order\", \"Dev\")\n",
    "\n",
    "f = open('results_rankings.txt', 'a')\n",
    "f.write(\"\\n\" \"-------------------------------------------------\")\n",
    "f.write(\"\\n\" + \"Evaluation on training data (rank image captions by similarity to original sentence):\")\n",
    "f.write(\"\\n\" + \"top1 accuracy \" +  str(top1accuracy(dataA_train[\"expected_order\"],  dataA_train[\"pred_order\"])))\n",
    "f.write(\"\\n\" +\"spearman correlation \" + str(spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order\"])))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_sentence\n",
      "top1 accuracy 0.257\n",
      "spearman correlation 0.11\n",
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_meaning\n",
      "top1 accuracy 0.386\n",
      "spearman correlation 0.183\n",
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_image\n",
      "top1 accuracy 0.514\n",
      "spearman correlation 0.143\n"
     ]
    }
   ],
   "source": [
    "for sent_type in ['sentence', 'meaning','image']:\n",
    "    print(\"\\nDepending on binary prediction (literal/idiomatic) rank images by similarity to gpt_\" + sent_type)\n",
    "    dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds(x,sent_type), axis=1)\n",
    "\n",
    "    dataA_train = only_train(dataA)\n",
    "    print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "    print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "    f = open('results_rankings.txt', 'a')\n",
    "    f.write(\"\\n\" \"-------------------------------------------------\")\n",
    "    f.write(\"\\n\" + \"\\nDepending on binary prediction (literal/idiomatic) rank images by similarity to gpt_\" + sent_type)\n",
    "    f.write(\"\\n\" + \"top1 accuracy \" +  str(top1accuracy(dataA_train[\"expected_order\"],  dataA_train[\"pred_order_dependent\"])))\n",
    "    f.write(\"\\n\" +\"spearman correlation \" + str(spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"])))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1 accuracy 0.514\n",
      "spearman correlation 0.143\n",
      "File zipped and saved as submission_EN.zip\n"
     ]
    }
   ],
   "source": [
    "sent_type = \"image\"\n",
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds(x,sent_type), axis=1)\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "make_submission(dataA,\"pred_order\", \"Dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim scores\n",
    "\n",
    "def dependent_preds_compare_pairs(current, sent_type):\n",
    "       preds = [0 for i in range(5)]\n",
    "       scores_lit = sim_scores(current, \"gpt_literal_\" + sent_type)\n",
    "       scores_id = sim_scores(current, \"gpt_idiomatic_\" + sent_type)\n",
    "       image_names = list(scores_lit.keys())\n",
    "       # get highest scoring image for literal and idiomatic\n",
    "       m_lit = max(scores_lit,key=scores_lit.get)\n",
    "       preds[0] = m_lit\n",
    "\n",
    "       del scores_id[m_lit]\n",
    "       m_id = max(scores_id, key=scores_id.get)\n",
    "       preds[3] = m_id\n",
    "    \n",
    "       m_lit_index = image_names.index(m_lit)\n",
    "       m_id_index = image_names.index(m_id)\n",
    "       scores_images_lit = sim_scores(current, \"image\" + str(m_lit_index + 1) + \"_caption\")\n",
    "       scores_images_id = sim_scores(current, \"image\" + str(m_id_index +1) + \"_caption\")\n",
    "       del scores_images_lit[m_lit]\n",
    "       del scores_images_lit[m_id]\n",
    "       del scores_images_id[m_lit]\n",
    "       del scores_images_id[m_id]\n",
    "\n",
    "       sim_max_lit = max(scores_images_lit, key=scores_images_lit.get)\n",
    "       preds[1] = sim_max_lit\n",
    "    \n",
    "       del scores_images_id[sim_max_lit]\n",
    "       sim_max_id = max(scores_images_id, key=scores_images_id.get)\n",
    "       preds[2] = sim_max_id\n",
    "       preds[4] = list(set(image_names).difference(set([m_lit,m_id,sim_max_lit,sim_max_id])))[0]\n",
    "       if not(set(preds) == set(image_names)):\n",
    "           print(\"there is some serious problem\") \n",
    "       if current[\"binary_pred\"] == \"idiomatic\":\n",
    "          preds_new = [preds[i] for i in [3,2,1,0,4]]\n",
    "          preds = preds_new\n",
    "       return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on training data (rank images dependent on binary classification. For order use inter-image similarity):\n",
      "top1 accuracy 0.543\n",
      "spearman correlation 0.381\n",
      "File zipped and saved as submission_EN.zip\n",
      "on extended data\n",
      "top1 accuracy 0.271\n",
      "spearman correlation 0.182\n"
     ]
    }
   ],
   "source": [
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds_compare_pairs(x,\"image\"), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"Evaluation on training data (rank images dependent on binary classification. For order use inter-image similarity):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "make_submission(dataA,\"pred_order_dependent\", \"Dev\")\n",
    "\n",
    "f = open('results_rankings.txt', 'a')\n",
    "f.write(\"\\n\" \"-------------------------------------------------\")\n",
    "f.write(\"\\n\" + \"Evaluation on training data (rank images dependent on binary classification. For order use inter-image similarity):\")\n",
    "f.write(\"\\n\" + \"top1 accuracy \" +  str(top1accuracy(dataA_train[\"expected_order\"],  dataA_train[\"pred_order_dependent\"])))\n",
    "f.write(\"\\n\" +\"spearman correlation \" + str(spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"])))\n",
    "f.close()\n",
    "\n",
    "\n",
    "dataA_trainext = dataA[dataA[\"sentence_type\"].notnull()] \n",
    "print(\"on extended data\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_trainext[\"expected_order\"], dataA_trainext[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_trainext[\"expected_order\"], dataA_trainext[\"pred_order_dependent\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim scores\n",
    "\n",
    "def dependent_preds_no_pairs(current, sent_type):\n",
    "    preds = [0 for i in range(5)]\n",
    "    scores_lit = sim_scores(current, \"gpt_literal_\" + sent_type)\n",
    "    scores_id = sim_scores(current, \"gpt_idiomatic_\" + sent_type)\n",
    "    image_names = list(scores_lit.keys())\n",
    "    # get highest scoring image for literal and idiomatic\n",
    "    m_lit = max(scores_lit,key=scores_lit.get)\n",
    "    preds[0] = m_lit\n",
    "\n",
    "    del scores_id[m_lit]\n",
    "    m_id = max(scores_id, key=scores_id.get)\n",
    "    preds[3] = m_id\n",
    "\n",
    "    del scores_lit[m_lit]\n",
    "    del scores_lit[m_id]\n",
    "\n",
    "    m_lit_second = max(scores_lit, key=scores_lit.get)\n",
    "    preds[1] = m_lit_second\n",
    "\n",
    "    del scores_id[m_id]\n",
    "    del scores_id[m_lit_second]\n",
    "    m_id_second = max(scores_id, key=scores_id.get)\n",
    "    preds[2] = m_id_second\n",
    "    preds[4] = list(set(image_names).difference(set([m_lit,m_id,m_lit_second,m_id_second])))[0]\n",
    "    if not(set(preds) == set(image_names)):\n",
    "           print(\"there is some serious problem\") \n",
    "    if current[\"binary_pred\"] == \"idiomatic\":\n",
    "          preds_new = [preds[i] for i in [3,2,1,0,4]]\n",
    "          preds = preds_new\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on training data (rank images dependent on binary classification. For order use similarity to gpt_image only):\n",
      "top1 accuracy 0.543\n",
      "spearman correlation 0.354\n",
      "File zipped and saved as submission_EN.zip\n",
      "on extended data\n",
      "top1 accuracy 0.271\n",
      "spearman correlation 0.182\n"
     ]
    }
   ],
   "source": [
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds_no_pairs(x,\"image\"), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"Evaluation on training data (rank images dependent on binary classification. For order use similarity to gpt_image only):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "make_submission(dataA,\"pred_order_dependent\", \"Dev\")\n",
    "\n",
    "f = open('results_rankings.txt', 'a')\n",
    "f.write(\"\\n\" \"-------------------------------------------------\")\n",
    "f.write(\"\\n\" + \"Evaluation on training data (rank images dependent on binary classification. For order use similarity to gpt_image only):\")\n",
    "f.write(\"\\n\" + \"top1 accuracy \" +  str(top1accuracy(dataA_train[\"expected_order\"],  dataA_train[\"pred_order_dependent\"])))\n",
    "f.write(\"\\n\" +\"spearman correlation \" + str(spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"])))\n",
    "f.close()\n",
    "\n",
    "print(\"on extended data\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_trainext[\"expected_order\"], dataA_trainext[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_trainext[\"expected_order\"], dataA_trainext[\"pred_order_dependent\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiomatic_literal_prediction(current, sent_type):\n",
    "    sims = model.similarity(current[\"sentence_sbert_embedding\"], [current[\"gpt_idiomatic_\" + sent_type + \"_sbert_embedding\"], current[\"gpt_literal_\" + sent_type +  \"_sbert_embedding\"]])\n",
    "    sims = sims.numpy()\n",
    "    if np.argmax(sims[0]) == 0:\n",
    "        return \"idiomatic\"\n",
    "    else:\n",
    "        return \"literal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy image 0.5428571428571428\n",
      "on extended accuracy image 0.5857142857142857\n",
      "accuracy sentence 0.7428571428571429\n",
      "on extended accuracy sentence 0.7642857142857142\n",
      "accuracy meaning 0.6285714285714286\n",
      "on extended accuracy meaning 0.7\n"
     ]
    }
   ],
   "source": [
    "f = open('results_rankings.txt', 'a')\n",
    "f.write(\"\\n\" \"-------------------------------------------------\")\n",
    "f.write(\"\\n\" + \"binary classification literal/idiomatic with SBERT embeddings:\")\n",
    "for sent_type in [\"image\", \"sentence\", \"meaning\"]:\n",
    "    dataA[\"binary_pred_sbert\"] = dataA.apply(lambda x: idiomatic_literal_prediction(x, sent_type), axis = 1)\n",
    "    dataA_train = only_train(dataA)\n",
    "    dataA_trainext = dataA[dataA[\"sentence_type\"].notnull()] \n",
    "    print(\"accuracy\",sent_type,accuracy_score(dataA_train[\"binary_pred_sbert\"], dataA_train[\"sentence_type\"]))\n",
    "    f.write(\"\\n\" + sent_type)\n",
    "    f.write(\"\\n\" + \"top1 accuracy \" +  str(top1accuracy(dataA_train[\"binary_pred_sbert\"],  dataA_train[\"sentence_type\"])))\n",
    "    print(\"on extended accuracy\",sent_type,accuracy_score(dataA_trainext[\"binary_pred_sbert\"], dataA_trainext[\"sentence_type\"]))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extended Evaluation    100\n",
       "Train                   60\n",
       "Dev                     15\n",
       "Test                    15\n",
       "Sample                  10\n",
       "Name: subset, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataA[\"subset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
